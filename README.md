# ğŸ¬ Lip Sync Video Generator

An AI-powered pipeline that transforms text into realistic lip-synced talking face videos using **ElevenLabs Text-to-Speech** and **Wav2Lip**.

Perfect for AI demos, virtual presenters, educational content, and speech-driven facial animation.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)

---

## ğŸ¥ What This Does

Transform a simple text script and a face image into a fully lip-synced video:

```text
ğŸ“ Text Script â†’ ğŸ™ï¸ AI Speech â†’ ğŸ‘„ Lip Sync â†’ ğŸ¬ Final Video
```

**Input:** Text + Face Image  
**Output:** Realistic talking face video with synchronized lips

---

## âœ¨ Features

- ğŸ—£ï¸ **Natural Speech Synthesis** - Powered by ElevenLabs TTS API  
- ğŸ‘„ **Accurate Lip Synchronization** - Using state-of-the-art Wav2Lip  
- ğŸ¤– **Smart Pipeline** - Auto-detects audio or script inputs  
- âš¡ **GPU Acceleration** - CUDA support for faster processing  
- ğŸ“‚ **Organized Workflow** - Clean input/output structure  
- ğŸš€ **One-Click Execution** - Run `main.py` and you're done  

---

## ğŸ§  Pipeline Overview

```text
[Input: Script or Audio] â†’ ğŸ¤– main.py (Auto-Pipeline) â†’ [Output: Lip-Synced Video]
```

---

## ğŸ“ Project Structure

```text
lip-sync-video-generator/
â”‚
â”œâ”€â”€ input/                               # Your input files
â”‚   â”œâ”€â”€ script.txt                       # Text to convert into speech
â”‚   â””â”€â”€ face.jpg                         # Face image (front-facing)
â”‚
â”œâ”€â”€ output/                              # Generated results
â”‚   â”œâ”€â”€ audio.wav                        # Generated speech audio
â”‚   â””â”€â”€ output_video.mp4                 # Final lip-synced video
â”‚
â”œâ”€â”€ Wav2Lip/                             # Wav2Lip model and scripts
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â””â”€â”€ wav2lip.pth                  # Wav2Lip model file
â”‚   â””â”€â”€ face_detection/detection/sfd/
â”‚       â””â”€â”€ s3fd.pth                     # Face detection model
â”‚
â”œâ”€â”€ main.py                              # ğŸš€ Unified Pipeline (Run this!)
â”œâ”€â”€ Elevenlab.py                         # Text-to-speech generator
â”œâ”€â”€ requirements.txt                     # Python dependencies
â”œâ”€â”€ .env                                 # API key (you create this)
â””â”€â”€ README.md
```

---

## ğŸ“¦ Requirements

| Requirement | Purpose |
|------------|---------|
| **Python 3.9+** | Core runtime |
| **FFmpeg** | Video processing |
| **ElevenLabs API Key** | Speech generation |
| **NVIDIA GPU + CUDA** *(Optional)* | Faster processing |

---

## ğŸï¸ Install FFmpeg (Windows)

```bash
winget install Gyan.FFmpeg
```

Restart your terminal after installation.

### Verify Installation

```bash
ffmpeg -version
```

---

## ğŸ› ï¸ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/Cl0ud-9/lip-sync-video-generator.git
```

Then open it in your code editor.

---

### 2ï¸âƒ£ Set Up a Virtual Environment

#### Create a Virtual Environment
```bash
python -m venv .venv
```
#### Activate Environment
```bash
.\.venv\Scripts\activate
```

---

### 3ï¸âƒ£ Install PyTorch

#### CPU Device Only
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

#### NVIDIA GPU Device (CUDA 12.1)
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

---

### 4ï¸âƒ£ Install Remaining Dependencies

```bash
pip install -r requirements.txt
```

---

### 5ï¸âƒ£ Configure ElevenLabs API Key

Get your API key from: 
https://elevenlabs.io/developers

Create a `.env` file in the root directory:

(Rename the `.env.example` file to `.env` and add your API key)

```
ELEVENLABS_API_KEY=your_api_key_here
```

---

## â¬‡ï¸ Download Required Model Files

### ğŸ”¹ Wav2Lip Model

Download:  
https://drive.google.com/uc?id=1fQtBSYEyuai9MjBOF8j7zZ4oQ9W2N64q  

Place here:

```
Wav2Lip/checkpoints/wav2lip.pth
```

---

### ğŸ”¹ Face Detection Model (S3FD)

Download:  
https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth

Rename to:

```
s3fd.pth
```

Place here:

```
Wav2Lip/face_detection/detection/sfd/s3fd.pth
```

---

## ğŸ¯ Usage (The Easy Way)

### 1ï¸âƒ£ Prepare Input
- **Face**: Put your image or video in `input/` (e.g., `input/face.jpg`).
- **Audio**:
    - **Option A (Text Script)**: Put your script in `input/script.txt`.
    - **Option B (Audio File)**: Put your audio in `input/audio.wav`.

### 2ï¸âƒ£ Run
```bash
python main.py
```
That's it! The script will automatically detect your input and generate the video.

**Output:** `output/output_video.mp4`

---

## ğŸ”§ Advanced / Manual Usage

If you want more control (like specific resize factors or specific file paths), you can run the scripts individually.

### ğŸ™ï¸ Step 1 â€” Generate Speech Audio (Optional)
```bash
python Elevenlab.py --script input/script.txt --output output/audio.wav
```

### ğŸ¬ Step 2 â€” Generate Lip-Synced Video
```bash
python Wav2Lip/inference.py --checkpoint_path Wav2Lip/checkpoints/wav2lip.pth --face input/image.jpg --audio input/audio.wav --outfile output/output_video.mp4 --resize_factor 2 --nosmooth --wav2lip_batch_size 256
```

---

## âš™ï¸ Verify PyTorch GPU Access

```bash
python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('Device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')"
```

---

## ğŸ› Troubleshooting

| Issue | Solution |
|------|----------|
| FFmpeg not found | Restart terminal |
| CUDA not detected | Install GPU PyTorch build |
| Blurry lips | Use a better face crop |
| Model not found | Check file paths |
| Slow processing | Use GPU |
| API key error | Verify `.env` file |

---

## ğŸ“œ Acknowledgements

- [Wav2Lip](https://github.com/Rudrabha/Wav2Lip)
- [ElevenLabs](https://elevenlabs.io)

---

## ğŸ“š Technical Overview

1. ElevenLabs generates speech  
2. S3FD detects face  
3. Wav2Lip generates lip motion  
4. FFmpeg renders final video  

---

## âš–ï¸ License

Licensed under the **MIT License** â€” see [LICENSE](LICENSE).

---

## ğŸ“Œ Disclaimer

For educational and research use only.  
Ensure consent before using any person's face or voice.

---

**Made with â¤ï¸ for the AI community**






